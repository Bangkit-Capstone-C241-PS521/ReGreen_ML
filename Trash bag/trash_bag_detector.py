# -*- coding: utf-8 -*-
"""Coba Sampah

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Meg3vlWssYbxQ56EhPxD_Z6oMXlCmSUJ
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

# Path ke dataset di Google Drive
dataset_path = '/content/drive/MyDrive/Trash Bag'

# Dapatkan daftar semua file gambar
all_images = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.jpg')]

# Bagi data menjadi training dan validation
train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)

# Buat direktori untuk menyimpan gambar sementara
os.makedirs('/tmp/train', exist_ok=True)
os.makedirs('/tmp/val', exist_ok=True)

# Pindahkan gambar ke direktori sementara
for img in train_images:
    os.symlink(img, os.path.join('/tmp/train', os.path.basename(img)))

for img in val_images:
    os.symlink(img, os.path.join('/tmp/val', os.path.basename(img)))

# Siapkan ImageDataGenerator untuk augmentasi data
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255)

# Menyiapkan train dan validation generator
train_generator = train_datagen.flow_from_directory(
    '/tmp',
    classes=['train'],
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary'
)

validation_generator = val_datagen.flow_from_directory(
    '/tmp',
    classes=['val'],
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary'
)

print(f"Number of training samples: {train_generator.samples}")
print(f"Number of validation samples: {validation_generator.samples}")

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Menggunakan model dasar MobileNetV2
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(150, 150, 3))

# Menambahkan lapisan tambahan
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x)

# Membuat model akhir
model = Model(inputs=base_model.input, outputs=predictions)

# Melakukan kompilasi model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Melatih model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    epochs=10
)

import numpy as np

# Fungsi untuk memprediksi berat berdasarkan area bounding box
def predict_weight(image_path, model, weight_per_pixel):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(150, 150))
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0) / 255.0

    prediction = model.predict(img_array)
    if prediction[0][0] > 0.5:
        # Jika terdeteksi kantong sampah, hitung berat
        bounding_box_area = 150 * 150  # Asumsi area bounding box (perlu diadaptasi sesuai model deteksi objek yang digunakan)
        estimated_weight = bounding_box_area * weight_per_pixel
        return estimated_weight
    else:
        return 0

# Contoh penggunaan fungsi prediksi berat
image_path = '/content/drive/MyDrive/Trash Bag/sample_image.jpg'
weight_per_pixel = 0.01  # Asumsi berat per pixel (perlu kalibrasi)
estimated_weight = predict_weight(image_path, model, weight_per_pixel)
print(f'Estimated weight of the trash bag: {estimated_weight} kg')

# Menyimpan model ke dalam file .h5
model_save_path = '/content/drive/MyDrive/trash_bag_detector_model.h5'
model.save(model_save_path)
print(f"Model saved to {model_save_path}")